# journal

## 2025-05-23

- got a simple version working
  - converges in 1500 epochs
  - relu activation function
  - normally distributed weights
  - L2 loss
  - network is \[9, 512, 512, 1\]
  - converges < 1e-6 in like 1500 epochs batch 128 (wait, 1024?)
- tried switching out for trainable gate activation function
  - very very slow to train -- relu is *fast*
  - after 1500 iters, loss hovers around 0.2, no dice
- tried fixed sparse weights, like original paper
  - e.g. generate identity matrix, shuffle it
  - loss hovers around 0.29, no dice
- what do predictions look like for relu weights, etc?
  - switching back to trainable layers and relu
  - after 1500, pretty close match between pred and actual
  - since we only use relu, clearly capped to 0 at bottom end
  - and then like 0.99 to 1.01 on top end
  - so certainly doing the right thing
- what about for relu but gate activation?
  - switching from relu to gate activation
  - where each of the gate functions is just relu on the first argument to the gate
  - so in theory this should converge
  - and indeed it does converge! loss < 1e-6 after just ~600 epochs
  - so the gate selection mechanism is fine
  - maybe the continuous relaxations of the logic gates we are using just aren't that non-linear?
- okay, what about gate activation, but only one is relu?
  - in theory, the network should be able to select for relu
  - in which case this should do no worse than the relu case
  - replaced first option, jnp.zeros_like(a) with relu
  - after 300 epochs, loss is 0.2
  - seems to be predicting ~0.3 regardless of input
  - after 1200 epochs, loss is still 0.2
  - seems to be predicting 0.3 regardless of input
- what are my options?
  - well, it looks like most of the gate activations aren't that non-linear
  - I could try using a different learning rate
  - I could try using a different loss function
  - I could try using a different optimizer
    - right now I'm just using batched sgd
    - but no momentum, etc.
  - I could try a simpler case
    - like gate with four function, one of them relu
  - I could try training for a lot longer
    - not likely to do anything if we're seeing no convergence
  - I could try speeding up the gate
    - this wouldn't really do anything
    - BUT it would make experimentation quicker
- let's see if the learning rate does anything
  - learning rate was at 0.1, I moved it to 0.01
    - after 500 epochs, loss 0.2
    - indiscriminately predicting 0.29 regardless of input
  - yeah, learning rate probably isn't the issue
    - if it were, we would expect e.g. loss to blow up or it to not be able to converge past a point
- let's try a lesser gate. how about 4 with one of them relu?
  - training with RELU, AND, XOR, OR
  - step size is 0.01
    - not converging after 1500 epochs (loss 0.2)
  - step size 0.1
    - wait, no, this actually started to converge
      - converged to loss 0.09 after 1500 epochs
  - since a bigger step size worked, we're able to traverse more of parameter space in the same number of epochs
  - likely had we let step size 0.01 run for 10x as long it would have started to converge to? let's try that, just to see
    - 15000 epochs at step size 0.01
      - after 500, loss is 0.2
      - after 1000, loss is 0.2 still
      - after 3000, loss is 0.17 (!)
      - after 5000, loss is 0.15-6 (nice)
      - after 15000, loss is 0.074
    - nice! so I learned:
      - if learning rate is stable over the loss landscape,
      - dividing rate by 10 will require 10x more time to train
      - but will train a little better, likely due to taking a shorter path over the loss landscape
- okay, so a few things could be true:
  - the paper works.
  - if so, if we train for a very very long time, it will converge
  - maybe regularization is very very important
  - I should re-read the paper to make sure I'm not missing anything
- I'm going to try to find info about network size, batch size, epochs, etc.
  - okay, so it looks like their network is ~48 gates wide by 20 gates deep
  - and yeah, they're using sparse connections, as I do
  - I could def speed up my impl if I used optimized sparse connections, but I really want to train with full weights and a sparsity constraint
  - from the graph, the conway's network with the above size dims looks to converge around 750 steps, and fully converge (e.g. find exact solution) by 1500 steps.
- I'm going to try some things:
  - Initialize gate vector to constant value 1/gates
  - I just tried it with network shape \[9, 48, 48, 1\]
    - batch size now 512
    - Converges with pure relu after 2500
    - Trying with mini gate (4 incl relu), converges after 2500 to < 0.0315, not bad
    - Trying with mini gate (AND, XOR, OR, NAND), NO relu:
      - after 2500: loss < 0.147
      - after 5000: loss < 0.08
      - after 9948: loss < 0.0139
      - then we get train_loss: inf, test_loss inf
        - and the network fails to converge
        - why the heck did that happen?
        - it was training so well ...
        - it looks like the last weight update resulted in very large outputs? compare:
          - Epoch (9947/15000) in 0.00447s
            - \[ 1.09918     0.03538388 -0.12122762 -0.16839814 -0.17385668\]
            - \[1. 0. 0. 0. 0.\]
          - Epoch (9948/15000) in 0.00408s;
            - \[-1.6563390e+30 -9.1450563e+27 -2.0778076e+33 -3.6336894e+32 -3.8909304e+31\]
            - \[0. 0. 1. 1. 0.\]
        - I don't have enough info to figure out why this happened but if numerical stability is a problem in the future I can fix it.
      - well, the good news is that the mini gate was working, but then blew up
        - seems to take like 5-10x as long as mini-gate incl relu
        - but good to know that whatever we're doing is just going to need a bit of training time to work
      - looking for potential zero-divs in code
        - gate_normalize has only div, used only to set the initial gate weights
    - Trying with full gate, for at least 10000 steps:
      - Unrelated, but I think a bit source of slowdown is displaying the accuracy stats, because they are not jit'd -- maybe I only show every say 100 epochs?
      - after 1500 epochs, loss is ~0.2
      - after 5000 epochs, loss is ~0.2 still
      - after 10000 epochs, loss is ~0.2 still
      - after 1500 epochs, loss is ~0.2 still UGH
      - so, full gate has a lot of trouble converging
        - maybe I need to add a regularizer term to force some gates? that might make the network have to pick something instead of just guessing an average?
        - maybe I should try to use as few gates as possible? e.g. just use AND, OR, maybe XOR, call it?
- I will implement a regularizer for the gates and weights now
  - first, I want to try 2-gate, just AND and OR:
    - 3000 epochs, loss < 0.15
    - 7000 epochs, loss < 0.08
    - 11577 loss < 0.0431
    - NOOOOO blow up at 11578
      - this was weird, the loss got big, then inf, then nan, over 3 epochs ... maybe learning rate is too high? or there are very big params in the network?
  - I will stick with the 4-gate, no relu version, because that converges the quickest so far
  - goal is to measure whether convergence happens more quickly
  - first, I try normalizing the gate weights to sum to 1:
    - I don't want to run this until full convergence, so
    - WAIT! it blew up. And this time only after 2590 epochs. never got lower than 0.17
      - I wonder if we are getting big weight terms because of a * b in e.g. AND and OR etc.?
      - then constraining the magnitude of the gate weights makes this blow up?
      - What if I also constrain the magnitude of the weight matrix?
  - I'm adjusting the step size to 0.05, wondering if that will help with the blow-ups
  - I'm trying to decide, should I constrain the weights during the update step, or as a penalty added to the loss term, like standard L1?
  - I tried clipping the weights between +/- 1:
    - loss gets down to 0.164 at epoch ~5000
    - then blows up to inf
  - so evidently these hard constraints mess with numeric stability, probably not the best idea
- I see the reference paper uses softmax to constrain the gate choice, maybe I should try that?
  - with softmax:
    - after 5000, loss is < 0.15
    - after 7000, loss is < 0.13
    - after **10000**, loss is 0.12
  - without, for comparison:
    - after 5000: loss is 0.161
    - after **10000**, loss is 0.0829
    - after 13000, loss is 0.0323
  - look, I didn't line them up exactly,
  - but it seems like softmax makes convergence a little slower?
  - Not a big deal if I force a gate. So I'll consider this problem solved
- So gates can be discretized, I want to regularize the weights
  - The idea is that each gate takes two inputs from the previous layer
  - We have a left matrix and a right matrix
  - So each matrix needs to "select" one gate from the previous layer
  - If that is the case, we could do like a softmax per layer (?) somehow. let's make that more concrete
    - consider Ax -> b
    - x is layer before, b is layer after
    - we dot x with the first row of A to produce the first entry in b
    - instead of a raw dot, we could softmax the first row of A before we dot, effectively normalizing the weights
    - so we need to compute a row-wise softmax of the weight matrix? would that be too slow?
    - testing with row-wise softmax, no gate-level softmax:
      - after 1300 epochs, loss is 0.151
        - that's fast! wowza!
      - 5000, loss is 0.159
        - hmmm, don't tell me it just plateaus
        - maybe the initial drop was due to being able to essentially softmax the last layer, like 0.15 is the new 0.20, but it's just going to stay flat
      - 10000, loss is still ~0.15
        - no dice it seems
    - I'm not going to give up, maybe we need a deeper network, since we're trying to force something sparser?
    - trying \[9, *(\[48\] * 20), 1\]:
      - oh gosh that's slow
      - wait, I think the slowdown really is just printing the loss
      - let me try removing that, ^C
      - now printing every 100 epochs, wow that made a difference -- about 4ms / epoch
      - bad news is, loss hovers around 0.2, as we've come to know.
    - so depth doesn't really help anything, at least for what we're dealing with.
- well, clipping the weights and softmaxing the weights doesn't work
  - maybe I'll try sparse dot with all the gates? especially since there's no e.g. negative now
    - now we're up to 13ms / epoch
    - I guess that makes sense e.g. 4 * 4ms = 16ms, right order of magnitude
    - epoch 5000, loss is 0.201
    - I don't think it will converge
- what about softmax weights + gate? maybe that's something?
  - I also changed the way weights were initialized, to be uniform
  - note with softmax the weights have no bias
  - still 13 ms / epoch
  - after 2000, loss is 0.214
  - I should be patient, I suppose
  - I will let this go to 15k
  - after 7000, loss is 0.206
  - after 10000, loss is 0.192
  - after 15000, loss is 0.207
  - yeah, really no progress, no dice
- so this softmax weights idea is pretty bad
  - I could try using a standard regularizer, like L1
  - I could try implementing a custom regularizer
  - I could try sparse wiring the right way, with fixed weights
    - but where's the novelty in that?
- I did a quick training run, \[9, 48, 48, 1\], standard weights (no softmax):
  - loss was ~0.2 after 15000, 3ms / epoch
  - Good news is training runs are faster
  - Bad news is 0.2 loss sucks
- Just for sanity's sake, with relu:
  - Gosh, convergence so quick
  - 1e-8 after 15000, man relu just cooks
  - why is relu so good?
- Lessons:
  - relu cooks
  - mini gate with weights + bias converges
  - mini gate can also converge with softmax, albeit slower
  - mini gate can blow up
  - constraining or clipping the weights sucks
- Future research:
  - can you get full gate working?
  - can you implement sparse fixed weights? does that work?
  - can you converge in 1500 steps like og paper?
    - what learning rate/optimizer do they use?
  - can you get constrained weights working?
  - can you save params to disk?
  - can you write out a discretized pure logic version of the program that runs quick, after training?
    - maybe like generate a c or zig program that uses SIMD
